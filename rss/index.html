<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[dorzey.net]]></title><description><![CDATA[My thoughts on Big Data, Machine Learning and Analytics]]></description><link>http://www.dorzey.net/</link><image><url>http://www.dorzey.net/favicon.png</url><title>dorzey.net</title><link>http://www.dorzey.net/</link></image><generator>Ghost 3.33</generator><lastBuildDate>Wed, 02 Dec 2020 22:23:02 GMT</lastBuildDate><atom:link href="http://www.dorzey.net/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[4 Interesting Blog Posts And 1 Podcast On Data Engineering]]></title><description><![CDATA[From data mesh to feature stores with some data qaulity and SQL mixed in.]]></description><link>http://www.dorzey.net/2020/12/02/5-interesting-data-engineering-blog-posts-ive-recently-read-2/</link><guid isPermaLink="false">5fc80de00ef36000015af3c5</guid><category><![CDATA[Reading List]]></category><category><![CDATA[Data Engineering]]></category><dc:creator><![CDATA[Paul Doran]]></dc:creator><pubDate>Wed, 02 Dec 2020 22:09:09 GMT</pubDate><media:content url="http://www.dorzey.net/content/images/2020/12/recently-read.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="blog-posts">Blog Posts</h3><ol><li><a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">Data Quality at Airbnb</a> - Airbnb detail how they've approached gaining traction on improving data quality.</li><li><a href="https://shopify.engineering/build-production-grade-workflow-sql-modelling">How to Build a Production Grade Workflow with SQL Modelling</a> - As a DBT user it's interesting to see how other companies are adopting the tool. Shopify's approach to materialize every model as a view on every commit is simple and powerful.</li><li><a href="https://www.thoughtworks.com/insights/blog/data-mesh-its-not-about-tech-its-about-ownership-and-communication">Data mesh: it's not about tech, it's about ownership and communication</a> - ThoughtWorks provide a nice overview framing the data mesh as a people problem.</li><li><a href="https://doordash.engineering/2020/11/19/building-a-gigascale-ml-feature-store-with-redis/">Building a Gigascale ML Feature Store with Redis, Binary Serialization, String Hashing, and Compression</a> - Feature Stores have been grabbing my attention recently. This post details how DoorDash scaled their Feature Store. It contains a comprehensive comparison of the approaches they tried before settling on Redis.</li></ol><h3 id="podcasts">Podcasts</h3><ol><li><a href="https://www.dataengineeringpodcast.com/bigeye-data-quality-market-episode-160/">Keeping A Bigeye On The Data Quality Market - Episode 160</a> - An interesting conversation covering the whole data quality space. Inlcuding the transition from data quality into data observability and data monitoring. I particularly liked the conversation contrasting proactive and reactive approaches.</li></ol>]]></content:encoded></item><item><title><![CDATA[5 Interesting Data Engineering Blog Posts I've Recently Read]]></title><description><![CDATA[I found all these worthwhile. They cover the breadth of Data Engineering. Let me know if you have anything that you think I would enjoy reading.]]></description><link>http://www.dorzey.net/2020/11/23/5-interesting-data-engineering-blog-posts-ive-recently-read/</link><guid isPermaLink="false">5fbbe57464ebcf0001e0a258</guid><category><![CDATA[Reading List]]></category><category><![CDATA[Data Engineering]]></category><dc:creator><![CDATA[Paul Doran]]></dc:creator><pubDate>Mon, 23 Nov 2020 17:12:33 GMT</pubDate><media:content url="http://www.dorzey.net/content/images/2020/11/recently-read.jpg" medium="image"/><content:encoded><![CDATA[<ol><li><a href="https://martinfowler.com/articles/productize-data-sci-notebooks.html">Don't put data science notebooks into production</a> - ThoughtWorks provide an interesting summary on the pain points of productionizing data science notebooks. I particularly like the focus on solving the people problems to empower teams to deliver.</li><li><a href="https://eng.uber.com/metadata-insights-databook/">Turning Metadata Into Insights with Databook</a> - A comprehesive overview from Uber on how leverage metadata in their Databook. I am hoping that they open source Dragon; their schema generation tool.</li><li><a href="https://greatexpectations.io/blog/data-tests-failed-now-what/">Your data tests failed! Now what?</a> - An outline of how you might go about operationalising data tests in your data pipelines.</li><li><a href="https://medium.com/df-foundation/meet-whale-the-stupidly-simple-data-discovery-tool-9f847c004b47">Meet whale! üê≥ The stupidly simple data discovery tool.</a> - Data discovery is a non-trivial problem and this new open source project offers a simple way to get started. I am looking forward to having a play.</li><li><a href="https://materialize.com/materialize-under-the-hood/">Materialize under the Hood</a> - Materialize is a SQL streaming database. This post provides a high level explanation of how it makes that happen. I am looking forward to going deeper and learning more about <a href="https://github.com/TimelyDataflow/timely-dataflow">timely dataflow</a> and <a href="https://github.com/TimelyDataflow/differential-dataflow">differential dataflow</a>.</li></ol>]]></content:encoded></item><item><title><![CDATA[Big Data LDN 2019: Taking control of user analytics with Snowplow]]></title><description><![CDATA[<figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/-gMuC1LCY50?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>User analytics is well established at Auto Trader but breakthroughs in the cloud, big data and streaming technologies offer clear benefits above and beyond what we could do today. This talk describes how we migrated to Snowplow on Google Cloud for our user-analytics platform. Snowplow is an event streaming pipeline</p>]]></description><link>http://www.dorzey.net/2019/12/16/big-data-ldn-2019-taking-control-of-user-analytics-with-snowplow/</link><guid isPermaLink="false">5f122464a8c34d000108b31f</guid><category><![CDATA[My Talks]]></category><category><![CDATA[Big Data]]></category><category><![CDATA[Analytics]]></category><dc:creator><![CDATA[Paul Doran]]></dc:creator><pubDate>Mon, 16 Dec 2019 12:00:00 GMT</pubDate><media:content url="http://www.dorzey.net/content/images/2020/07/tmp-2.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/-gMuC1LCY50?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><img src="http://www.dorzey.net/content/images/2020/07/tmp-2.png" alt="Big Data LDN 2019: Taking control of user analytics with Snowplow"><p>User analytics is well established at Auto Trader but breakthroughs in the cloud, big data and streaming technologies offer clear benefits above and beyond what we could do today. This talk describes how we migrated to Snowplow on Google Cloud for our user-analytics platform. Snowplow is an event streaming pipeline that provides many features such as a unified log of your users, event enrichments and a schema registry to enforce data integrity. I'll walk through why we chose to migrate, the overall Snowplow platform, the architectural benefits and how using Google BigQuery and DataFlow has been a huge success for Auto Trader.</p>]]></content:encoded></item><item><title><![CDATA[Continuous deployment of machine learning models]]></title><description><![CDATA[<figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/wnx5yYVf2hQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p> This talk describes how Auto Trader launched a suite of new machine learning models with the ability serve low-latency predictions in real time. These models are automatically retrained and redeployed using continuous deployment pipelines in our existing deployment infrastructure, making use of technology including Apache Spark, Airflow, Docker and Kubernetes.</p>]]></description><link>http://www.dorzey.net/2019/05/16/continuous-deployment-of-machine-learning-models/</link><guid isPermaLink="false">5f122426a8c34d000108b315</guid><category><![CDATA[My Talks]]></category><category><![CDATA[ML]]></category><category><![CDATA[Big Data]]></category><dc:creator><![CDATA[Paul Doran]]></dc:creator><pubDate>Thu, 16 May 2019 12:00:00 GMT</pubDate><media:content url="http://www.dorzey.net/content/images/2020/07/tmp-1.png" medium="image"/><content:encoded><![CDATA[<figure class="kg-card kg-embed-card"><iframe width="612" height="344" src="https://www.youtube.com/embed/wnx5yYVf2hQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><img src="http://www.dorzey.net/content/images/2020/07/tmp-1.png" alt="Continuous deployment of machine learning models"><p> This talk describes how Auto Trader launched a suite of new machine learning models with the ability serve low-latency predictions in real time. These models are automatically retrained and redeployed using continuous deployment pipelines in our existing deployment infrastructure, making use of technology including Apache Spark, Airflow, Docker and Kubernetes. Since models are deployed without manual intervention, we developed a robust testing strategy to ensure deployments will not cause a drop in model performance, including accuracy and coverage.</p>]]></content:encoded></item><item><title><![CDATA[How we use Architectural Decision Records (ADRs) on Data Engineering]]></title><description><![CDATA[<p>When I joined our data engineers over a year ago they had already adopted Architectural Decision Records (ADRs) to document architectural decisions made whilst building Auto Trader‚Äôs Data Platform. ADRs are listed in ThoughtWorks‚Äô <a href="https://www.thoughtworks.com/radar/techniques/lightweight-architecture-decision-records">Technology Radar</a> as a technique to adopt; our data engineers were the first team I‚Äô</p>]]></description><link>http://www.dorzey.net/2019/03/20/how-we-use-architectural-decision-records-adrs-on-data-engineering/</link><guid isPermaLink="false">5f122507a8c34d000108b32b</guid><category><![CDATA[Ways Of Working]]></category><dc:creator><![CDATA[Paul Doran]]></dc:creator><pubDate>Wed, 20 Mar 2019 12:00:00 GMT</pubDate><media:content url="http://www.dorzey.net/content/images/2020/07/tmp-3.png" medium="image"/><content:encoded><![CDATA[<img src="http://www.dorzey.net/content/images/2020/07/tmp-3.png" alt="How we use Architectural Decision Records (ADRs) on Data Engineering"><p>When I joined our data engineers over a year ago they had already adopted Architectural Decision Records (ADRs) to document architectural decisions made whilst building Auto Trader‚Äôs Data Platform. ADRs are listed in ThoughtWorks‚Äô <a href="https://www.thoughtworks.com/radar/techniques/lightweight-architecture-decision-records">Technology Radar</a> as a technique to adopt; our data engineers were the first team I‚Äôve worked on to use them. They have allowed us to capture the context and consequences of the decisions we make; in a way that provides transparency and allows the whole team, and wider organisation, to contribute.</p><h2 id="what-is-an-adr">What is an ADR?</h2><p>ADRs are short text files that capture a single decision; see <a href="http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions">here</a> for a more in-depth description. The ADRs are numbered sequentially and monotonically; with no number reused.</p><p>We have adopted the below convention for our ADRs:</p><ul><li><strong>Title</strong>: A descriptive title in the following format: <code>&lt;ADR_ID - &lt;Title&gt;</code></li><li><strong>Status</strong>: It must have one of the below statuses.</li></ul><figure class="kg-card kg-image-card"><img src="http://engineering.autotrader.co.uk/images/2019-03-20/adr_state_diag.svg" class="kg-image" alt="How we use Architectural Decision Records (ADRs) on Data Engineering"></figure><p>ADRs can only change status along the permitted transitions shown in the diagram above.</p><ul><li><strong>Context</strong>: The facts about why we need to make this decision. For example, they could be technological, such as we need to provide a certain capability; or they could be legal, such as compliance requirements.</li><li><strong>Decision</strong>: The actual decision. We provide a description of what we decided and provide justifications if needed. The decision should be an action we will undertake.</li><li><strong>Consequences</strong>: We provide a description of all the potential consequences; be they good, bad or neutral.</li></ul><h2 id="how-are-adrs-used-in-data-engineering">How are ADRs used in Data Engineering?</h2><p>Our data engineers maintain a website in <a href="https://jekyllrb.com/docs/github-pages/">GitHub Pages using Jekyll</a>. This was already used to share common links, jargon bust and build statuses - so the ADRs were placed there too. The squad website is readable by anyone at Auto Trader and contributions are welcome from anyone.</p><p>There are ADRs covering a wide range of subjects; how to partition data in the data lake, choice of business intelligence tool, proposed changes to metric algorithms, etc.</p><p>Having a place to record the ADRs is not enough. Consensus needs to be reached on whether ADRs should change status or not. We either address this at standup or with specific meetings for more in-depth discussions.</p><p>As the Data Platform was built lots of architectural decisions were made. We‚Äôve found it worthwhile revisiting the decisions and see if they still hold; given <a href="https://engineering.autotrader.co.uk/2018/11/21/lessons-from-the-data-lake-architectural-decisions.html">what we‚Äôve learnt</a> as the platform grows.</p><h3 id="benefits-of-adrs">Benefits of ADRs</h3><h4 id="one-clearly-reasoned-decisions">One: Clearly reasoned decisions</h4><p>The format of the ADRs enforces consistency in approach to documenting decisions. This makes it easier for both the author and the reader. The author knows what information to provide to produce an ADR, and the reader knows what to expect when reading an ADR and where to find it.</p><p>As the Data Platform is being built new technology is continuously adopted. The ADRs allow us to show stakeholders that we are in control of technology adoption and change.</p><h4 id="two-good-for-new-team-members">Two: Good for new team members</h4><p>I found the ADRs invaluable when I first joined Data Engineering. They allowed me to get up to speed quickly on why the team had made certain decisions. I could read the ADRs to gain a broad understanding of the architecture and to understand the context of the decision. The ADRs told me the history of the decisions made by Data Engineering.</p><h4 id="three-version-control-for-the-history-of-team-decisions">Three: Version control for the history of team decisions</h4><p>Given we are using GitHub Pages to store our ADRs we get a versioned history of their changes for free. I have found it useful to review the history of an ADR to understand its evolution or to know who to ask for clarification if something is unclear.</p><h4 id="four-keeping-them-lightweight">Four: Keeping them lightweight</h4><p>We don‚Äôt embrace ‚Äòbig design up front‚Äô so the ADRs are not mammoth documents that nobody will ever read. The theme used for Jekyll has a built-in reading time estimator; the longest ADR to date has an estimated reading time of 4 minutes.</p><h3 id="challenges-of-adrs">Challenges of ADRs</h3><h4 id="one-deciding-when-to-create-one">One: Deciding when to create one</h4><p>There are no defined rules for when to create an ADR. Individuals are free to propose an ADR when they feel one is necessary. The hope being that by doing this we are capturing all the ADRs needed. I think over time, and with experience, we‚Äôve gotten better at ensuring ADRs are written when required. There have been only a few occasions where I have felt that an ADR was missed.</p><h4 id="two-keeping-them-up-to-date">Two: Keeping them up-to-date</h4><p>The environment our data engineers work in is reasonably fast paced and lots of decisions can get made in a short space of time. I have found, on occasion, that the ADRs are out of date.</p><p>If we fail to write them in a timely manner then we reduce the value of the ADRs; future potential readers have no way of knowing what was not recorded. This was one of the reasons we now have semi-regular catch-ups to discuss ADRs.</p><h4 id="three-keeping-them-lightweight">Three: Keeping them lightweight</h4><p>The preference for keeping the ADRs lightweight can make them challenging to write. When writing an ADR you want to provide enough information to a future reader, but do it in a concise way. This requires good clear writing; which isn‚Äôt the easiest thing to achieve when documenting a detailed technical decision.</p><h2 id="conclusions">Conclusions</h2><p>We‚Äôve found great value in ADRs as both a historical record of decisions and a way to capture the thinking behind architectural choices. Their number continues to grow.</p><p>They were incredibly useful to me when I first joined and I continue to find them a valuable tool.</p>]]></content:encoded></item></channel></rss>